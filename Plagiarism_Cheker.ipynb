{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import docx\n",
    "import datetime\n",
    "from docx.shared import Inches\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step # 1\n",
    "def step_1_find_files_extension(extension,path):\n",
    "    os.getcwd()\n",
    "    all_files = os.listdir(path)\n",
    "    files = [file for file in all_files if file.lower().endswith((extension))]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "def step_2_read_pdf_files(files,path):\n",
    "    # a list that contain all the str(file texts)\n",
    "    list_of_files = []\n",
    "    # opening all pdf files in the current directory\n",
    "    for pdf_file in files:\n",
    "        file = open(path+pdf_file,'rb')\n",
    "        # reading those files\n",
    "        readed_file = PdfFileReader(file)\n",
    "        # checking if file is encrypted\n",
    "        if not readed_file.isEncrypted:\n",
    "            # a str to save all text of pdf file \n",
    "            file_1 = ' '\n",
    "            # counting total pages in the pdf file\n",
    "            total_pages = readed_file.getNumPages()\n",
    "#           print(f\"Total no of Pages = {total_pages} in {pdf_file}\")\n",
    "            # extracting text from every page one by one \n",
    "            for page in range(total_pages):\n",
    "                # getting page number \n",
    "                page_obj = readed_file.getPage(page)\n",
    "                # extracting page and save them in a str\n",
    "                file_1 += page_obj.extractText()\n",
    "               # print(file_1)\n",
    "            # saveing the complete text of file into a list\n",
    "            list_of_files.append(file_1)\n",
    "            file.close()\n",
    "        else:\n",
    "            print('File is Locked')\n",
    "    return list_of_files\n",
    "def step_2_read_word_files(files,path):\n",
    "    list_of_files_2 = []\n",
    "    for file in files:\n",
    "        file_text = ''\n",
    "        doc = docx.Document(path+file)\n",
    "        for i in doc.paragraphs:\n",
    "            file_text += i.text\n",
    "        list_of_files_2.append(file_text)\n",
    "    return list_of_files_2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Includes\n",
    "\n",
    "# 1: remove Punctuations \n",
    "# 2: Tokanization\n",
    "# 3: remove stop words\n",
    "# 6: remove two-length words\n",
    "# 4: stemming\n",
    "# 5: lemmatization\n",
    "# using re and nltk\n",
    "\n",
    "\n",
    "def text_cleaner(string):\n",
    "    #remove \\n\n",
    "    clean0 = string.replace('\\n','')\n",
    "    # remove punc\n",
    "    clean1 = re.sub('[^a-zA-Z]',' ',clean0)\n",
    "    # tokenization\n",
    "    words = word_tokenize(clean1)\n",
    "    # removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean4 = [w for w in words if not w in stop_words]\n",
    "    # two-length words\n",
    "    clean5 = [w for w in clean4 if len(w) > 2]\n",
    "    # normalization\n",
    "    clean6 = [word.lower() for word in clean5]\n",
    "    # stemming\n",
    "#     stemmer = PorterStemmer()\n",
    "#     clean7 = [stemmer.stem(word) for word in clean6]\n",
    "    # lemmatization\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    clean8_list = [lemmer.lemmatize(w) for w in clean6]\n",
    "    return clean8_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning all the files in the list of files\n",
    "def step_3_clean_all_files(list_of_files):\n",
    "    clean_list_of_files = [text_cleaner(file_) for file_ in list_of_files]\n",
    "    return clean_list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_name_files_with_student_names(files_names,cleaned_files):\n",
    "    cleaned_pdf_files_with_student_names = {}\n",
    "    for i in range(len(files_names)):\n",
    "        cleaned_pdf_files_with_student_names[files_names[i]] = cleaned_files[i]\n",
    "    return cleaned_pdf_files_with_student_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matched_words_and_synonyms(student1,cleaned_text_1,student2,cleaned_text_2,plague_threshould = 0.2):    \n",
    "    matched_words = {}\n",
    "    matched_synonyms = {}\n",
    "    cleaned_text_1_unique = list(set(cleaned_text_1))\n",
    "    cleaned_text_2_unique = list(set(cleaned_text_2))\n",
    "#     if len(cleaned_text_1_unique) <= len(cleaned_text_2_unique):\n",
    "    for word1 in cleaned_text_1_unique:\n",
    "        for word2 in cleaned_text_2_unique:\n",
    "            if word1 == word2:\n",
    "                matched_words[word1] = word2 \n",
    "            else:\n",
    "                for syn in wordnet.synsets(word2): \n",
    "                    if syn.lemmas():\n",
    "                        for l in syn.lemmas(): \n",
    "                            if word1 == l.name() and word2 != l.name():\n",
    "                                matched_synonyms[word1] = word2\n",
    "\n",
    "        \n",
    "        wplagued = len(matched_words.keys()) / len(cleaned_text_1_unique) \n",
    "        splagued = len(matched_synonyms.keys()) / len(cleaned_text_1_unique)\n",
    "#         tplagued = wplagued * splagued \n",
    "     \n",
    "#     if len(cleaned_text_1_unique) > len(cleaned_text_2_unique):\n",
    "#         for word1 in cleaned_text_2_unique:\n",
    "#             for word2 in cleaned_text_1_unique:\n",
    "#                 if word1 == word2:\n",
    "#                     matched_words[word1] = word2 \n",
    "#                 else:\n",
    "#                     for syn in wordnet.synsets(word2): \n",
    "#                         if syn.lemmas():\n",
    "#                             for l in syn.lemmas(): \n",
    "#                                 if word1 == l.name() and word2 != l.name():\n",
    "#                                     matched_synonyms[word1] = word2\n",
    "                                    \n",
    "#         wplagued = len(matched_words.keys()) / len(cleaned_text_2_unique) \n",
    "#         splagued = len(matched_synonyms.keys()) / len(cleaned_text_2_unique)\n",
    "# #         tplagued = wplagued * splagued\n",
    "        \n",
    "    if wplagued >= plague_threshould:\n",
    "        return {'Student_1':student1,'Student_2':student2,\n",
    "                'Word Matched Plagued(%)': wplagued * 100 ,'Synonyms Matched Plagued(%)': splagued * 100 ,\n",
    "                'Matched_words': matched_words,'Matched_Synonyms':matched_synonyms} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_5_find_plagues(cleaned_file_with_student_name, plague_threshould = 0.2):\n",
    "    result = []\n",
    "    prev_stud_1 = None\n",
    "    prev_stud_2 = None\n",
    "    for student1,compare_from_file in cleaned_file_with_student_name.items():\n",
    "        for student2,compare_to_file in cleaned_file_with_student_name.items():\n",
    "            if student1 != student2 and prev_stud_1 != student2 and prev_stud_2 != student1:\n",
    "                prev_stud_1,prev_stud_2 = student1,student2\n",
    "                res = find_matched_words_and_synonyms(student1,compare_from_file,student2,compare_to_file,\n",
    "                                                             plague_threshould=plague_threshould)\n",
    "                if res:\n",
    "                    result.append(res)\n",
    "    if not result:\n",
    "        print(f\"No Student did {plague_threshould * 100} % Plagued\")\n",
    "        return result\n",
    "    else:\n",
    "        return result            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_6_make_plagued_detailed_report_of_all_students(result_pdf,result_docx):\n",
    "    path1 = \"Plagiarism_Reports\"\n",
    "    if not os.path.isdir(\"Plagiarism_Reports\"):\n",
    "        os.mkdir(path1)\n",
    "    if result_pdf and not result_docx:\n",
    "        result_docx = dict()\n",
    "    elif result_docx and not result_pdf:\n",
    "        result_pdf = dict()\n",
    "    elif not result_pdf and not result_docx:\n",
    "        result_docx = dict()\n",
    "        result_pdf = dict()\n",
    "        \n",
    "    df_pdf = pd.DataFrame(result_pdf)\n",
    "    df_doc = pd.DataFrame(result_docx)\n",
    "    df_complete = pd.concat([df_pdf,df_doc])\n",
    "    df_complete.reset_index(inplace=True)\n",
    "    df_complete.drop('index',axis=1,inplace=True)\n",
    "    df_complete.to_csv(\"Plagiarism_Reports/Detailed_Plagued_Report.csv\")\n",
    "    print(\"Success! Detailed Report has been created in  Plagiarism_Reports  directry\")\n",
    "    return df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_student_file(**kwargs):\n",
    "    \n",
    "    document = docx.Document()\n",
    "    now = datetime.datetime.now()\n",
    "    time = now.strftime(\"%Y-%m-%d %I:%M-%p\")\n",
    "    document.add_heading(f'Plagued Report',level = 0)\n",
    "    document.add_heading(f\"Processed on :\",level=1)\n",
    "    document.add_paragraph(f\"{time}\")\n",
    "\n",
    "    document.add_heading(f\"Student Name (who plagued) :\",level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Student_1']}\")\n",
    "\n",
    "    document.add_heading(f\"Student Name (whom plagued) :\",level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Student_2']}\")                         \n",
    "\n",
    "    document.add_heading(f\"Word Matched Plagued in percentage :\",level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Word Matched Plagued(%)']}\")\n",
    "\n",
    "    document.add_heading(f\"Synonyms Matched Plagued in percentage :\",level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Synonyms Matched Plagued(%)']}\")\n",
    "\n",
    "    document.add_heading('Matched words:', level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Matched_words']}\") # , style='Intense Quote')\n",
    "\n",
    "    document.add_heading('Total Matched words:', level=1)\n",
    "    document.add_paragraph(f\"{len(kwargs['Matched_words'])}\") # , style='Intense Quote')\n",
    "\n",
    "    document.add_heading('Matched Synonyms:', level=1)\n",
    "    document.add_paragraph(f\"{kwargs['Matched_Synonyms']}\") # , style='Intense Quote')\n",
    "\n",
    "    document.add_heading('Total Matched Synonyms:', level=1)\n",
    "    document.add_paragraph(f\"{len(kwargs['Matched_Synonyms'])}\") # , style='Intense Quote')\n",
    "    document.save(f'Plagiarism_Files/{kwargs[\"Student_1\"]}.docx')\n",
    "\n",
    "# document.add_paragraph(\n",
    "#     'first item in unordered list', style='List Bullet'\n",
    "# )\n",
    "# document.add_paragraph(\n",
    "#     'first item in ordered list', style='List Number'\n",
    "# )\n",
    "\n",
    "# records = (\n",
    "#     (3, '101', 'Spam'),\n",
    "#     (7, '422', 'Eggs'),\n",
    "#     (4, '631', 'Spam, spam, eggs, and spam')\n",
    "# )\n",
    "\n",
    "# table = document.add_table(rows=1, cols=3)\n",
    "# hdr_cells = table.rows[0].cells\n",
    "# hdr_cells[0].text = 'Qty'\n",
    "# hdr_cells[1].text = 'Id'\n",
    "# hdr_cells[2].text = 'Desc'\n",
    "# for qty, id, desc in records:\n",
    "#     row_cells = table.add_row().cells\n",
    "#     row_cells[0].text = str(qty)\n",
    "#     row_cells[1].text = id\n",
    "#     row_cells[2].text = desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_7_make_plagued_brief_report_of_all_students(detailed_report):\n",
    "    if not os.path.isdir(\"Plagiarism_Reports\"):\n",
    "        os.mkdir(path1)\n",
    "    highest_df = dict(detailed_report.groupby(['Student_1'])['Word Matched Plagued(%)'].max())\n",
    "    name,plagued = list(highest_df.keys()),list(highest_df.values())\n",
    "    data = pd.DataFrame([])\n",
    "    for nam,key in zip(name,plagued):\n",
    "        df = pd.DataFrame(data=detailed_report[(detailed_report['Student_1'] == nam) & (detailed_report['Word Matched Plagued(%)'] == key)])\n",
    "        data = data.append(df , ignore_index=True)\n",
    "    data.to_csv(\"Plagiarism_Reports/brief_Plagued_Report.csv\")\n",
    "    print(\"Success! Brief Plagued Report has been created in your current directry\")    \n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students,datasets = [],[]\n",
    "#     students.append('Student_1')\n",
    "#     students.append('Student_2')\n",
    "#     path1 = \"Plagiarism_Reports\"\n",
    "#     if not os.path.isdir(\"Plagiarism_Reports\"):\n",
    "#         os.mkdir(path1)\n",
    "#     for student in students:\n",
    "#         highest_df = dict(detailed_report.groupby([student])['Word Matched Plagued'].max())\n",
    "#         name,plagued = list(highest_df.keys()),list(highest_df.values())\n",
    "#         data = pd.DataFrame([])\n",
    "#         for nam,key in zip(name,plagued):\n",
    "#             df = pd.DataFrame(data=detailed_report[(detailed_report[student] == nam) & (detailed_report['Word Matched Plagued'] == key)])\n",
    "#             data = data.append(df , ignore_index=True)\n",
    "#         data.to_csv(f\"Plagiarism_Reports/brief_Plagued_Report_of_{student}.csv\")\n",
    "#         datasets.append(data)\n",
    "#     return datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_make_plagued_file_of_individual_students(brief_report):\n",
    "    count = 0\n",
    "    path_ = \"Plagiarism_Files\"\n",
    "    if not os.path.isdir(\"Plagiarism_Files\"):\n",
    "        os.mkdir(path_)       \n",
    "    for i in range(len(brief_report)):\n",
    "        one_row = dict(brief_report.iloc[i,:])\n",
    "        create_student_file(**one_row)\n",
    "        count += 1 \n",
    "    return f\"{count} files has been created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Student did 0.0 % Plagued\n"
     ]
    }
   ],
   "source": [
    "# find files in current directory\n",
    "files_pdf = step_1_find_files_extension('.pdf',path = \"students_files/\")\n",
    "# # reading files\n",
    "readed_pdf_files = step_2_read_pdf_files(files_pdf, path =\"students_files/\")\n",
    "#  cleaning files\n",
    "cleaned_pdf_files = step_3_clean_all_files(readed_pdf_files)\n",
    "\n",
    "# name files with student names\n",
    "# pdf files with student names\n",
    "cleaned_files_with_names = step_4_name_files_with_student_names(files_names=files_pdf,cleaned_files=cleaned_pdf_files)\n",
    "\n",
    "# find plages of these files\n",
    "result_pdf =  step_5_find_plagues(cleaned_files_with_names,plague_threshould=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find files in current directory\n",
    "files_docx = step_1_find_files_extension(('docx'),path=\"students_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reading files\n",
    "readed_docx_files = step_2_read_word_files(files_docx,path = \"students_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cleaning files \n",
    "cleaned_docx_files = step_3_clean_all_files(readed_docx_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name files with student names\n",
    "# docx files with student names\n",
    "cleaned_files_with_names = step_4_name_files_with_student_names(files_names=files_docx,cleaned_files=cleaned_docx_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find plages of these files\n",
    "# result_docx =  step_5_find_plagues(cleaned_files_with_names,plague_threshould=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docx =  step_5_find_plagues(cleaned_files_with_names,plague_threshould=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Detailed Report has been created in  Plagiarism_Reports  directry\n"
     ]
    }
   ],
   "source": [
    "detailed_report = step_6_make_plagued_detailed_report_of_all_students(result_pdf,result_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Brief Plagued Report has been created in your current directry\n"
     ]
    }
   ],
   "source": [
    "brief_report = step_7_make_plagued_brief_report_of_all_students(detailed_report=detailed_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_files = step_8_make_plagued_file_of_individual_students(brief_report=brief_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21 files has been created'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_df = dict(detailed_report.groupby(['Student_2'])['Word Matched Plagued'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name,plagued = list(highest_df.keys()),list(highest_df.values())\n",
    "data_2 = pd.DataFrame([])\n",
    "for nam,key in zip(name,plagued):\n",
    "    df = pd.DataFrame(data=detailed_report[(detailed_report['Student_2'] == nam) & (detailed_report['Word Matched Plagued'] == key)])\n",
    "    data_2 = data_2.append(df , ignore_index=True)\n",
    "# data.to_csv(\"brief_Plagued_Report_of_Who_Plagued.csv\")\n",
    "# print(\"Success! Brief Plagued Report has been created in your current directry\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Plagiarism_Reports/Detailed_Plagued_Report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.groupby(['Student_1'])['Word Matched Plagued'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "highest_df = dict(detailed_report.groupby(['Student_1'])['Word Matched Plagued'].max())\n",
    "name,plagued = list(highest_df.keys()),list(highest_df.values())\n",
    "data = pd.DataFrame([])\n",
    "for nam,key in zip(name,plagued):\n",
    "    df = pd.DataFrame(data=detailed_report[(detailed_report['Student_1'] == nam) & (detailed_report['Word Matched Plagued'] == key)])\n",
    "    data = data.append(df , ignore_index=True)\n",
    "data.to_csv(\"Plagiarism_Reports/brief_Plagued_Report.csv\")\n",
    "print(\"Success! Brief Plagued Report has been created in your current directry\")    \n",
    "return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in result_pdf:\n",
    "#     for name,value in result.items():\n",
    "#         print(f\"{name} -> {value}\")\n",
    "#     print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in result_docx:\n",
    "    for name,value in result.items():\n",
    "        print(f\"{name} -> {value}\")\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_4_prepare_dataset(result_pdf=result_pdf,result_docx=result_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = '''Computer Networks\n",
    "Assignment 1:\n",
    "1. What is a network protocol? Explain why are standards important for protocols?\n",
    "\n",
    "Network Protocols:\n",
    "Network protocols are formal standards and rules, procedures and formats that define communication between two or more devices over a network.\n",
    "Standards are important for protocols as they are rules which are used by different devices to communicate effectively without said rules devices would not be able to communicate with each other.\n",
    "\n",
    "2. Differentiate between unicast, multicast and broadcast in computer networks. Give an example of a situation in which multicast addresses might be beneficial.\n",
    "Unicast:\n",
    "It refers to such a transmission on a network where there is only one sender and receiver. It is a one-to-one transmission from one end of the network to the other.\n",
    "Multicast:\n",
    "In multicast there are one or more senders and one or more receivers on a network. In this network traffic follows either one to all or all to one transmission.\n",
    "Broadcast:\n",
    "Broadcasting is a one to all transmission method there are different types of broadcasts.\n",
    "Limited Broadcasting: \n",
    "\tLimited broadcast is the broadcast limited to a single LAN and which is to be received by all.\n",
    "Direct Broadcasting:\n",
    "\tThis is useful when a device in one network wants to transfer packet stream to all the devices over the other network.\n",
    "\n",
    "3. For each of the following four networks, discuss the consequences if a connection fails.\n",
    "a. Five devices arranged in a mesh topology\n",
    "Only one computer will go offline from network\n",
    "b. Five devices arranged in a star topology (not counting the hub)\n",
    "Only one computer goes offline from network\n",
    "c. Five devices arranged in a bus topology\n",
    "if one computer fails it will not affect the network the failed computer wont be able to send any packets.\n",
    "D. Five devices arranged in a ring topology \n",
    "If one computer fails in a ring topology then all computers stop receiving data as it has to go through the failed computer.\n",
    "\n",
    "4. What is Hub, switch and Router? Differentiate between their usages in a network. Explain your answer with an example.\n",
    "Hub:\n",
    "A device which is used to connect different computers on a network. It is mostly used in local area networks where there is small number of computers. The hub does not have any software on it. It is only hardware.\n",
    "Switch:\t\n",
    "A switch is a device which connects different computers but it has software on board. A switch used a switching table which has mac addresses of the computers connected to it which it uses to identify each computer. Also used in local area networks such as labs.\n",
    "Router:\n",
    "A router is device which is used to connect multiple networks it is the more advanced of the last two devices as it is used everywhere. A router has a routing table which has ip addresses of the computers on the network which is used to identify each computer instead of mac address uniquely. Routers are widely used and currently the internet uses routers to connect different networks.\n",
    "\n",
    "5. Compare the OSI and TCP/IP model. Give at least one example of each.\n",
    "OSI Model:\n",
    "OSI stands for open systems interconnection model.\n",
    "It is more of a theoretical reference model that is used to guide developers so the software programs they create can inter operate, and to facilitate a clear framework that describes the functions of a networking system. It has two additional layers than tcp/ip model.\n",
    "OSI model has seven layers that are:\n",
    "•\tApplication\n",
    "•\tPresentation\n",
    "•\tSession\n",
    "•\tTransmission\n",
    "•\tNetwork\n",
    "•\tData-link\n",
    "•\tPhysical\n",
    "\n",
    "TCP/IP Model:\n",
    "\tTCP/IP model stands for transfer control protocol/Internet protocol.\t\n",
    "Unlike OSI model this model only has 5 layers excluding the two additional layers in OSI.\n",
    "TCP/IP is widely used instead of OSI. \n",
    "TCP/IP has five layers which are:\n",
    "•\tApplication Layer\n",
    "•\tTransport Layer\n",
    "•\tNetwork Layer\n",
    "•\tData-Link Layer\n",
    "•\tPhysical Layer\n",
    "6. What are end systems in a network? Why are they called hosts? List several different types of end systems?\n",
    "\tEnd systems are the systems that are on the edge of a network.\n",
    "\tEnd systems are also called hosts because they host networking applications.\n",
    "\tDifferent types of end systems include Laptops, Computers and Mobiles etc.\n",
    "\n",
    "7. Differentiate between DSL and Cable Networks. Suppose both the networks use same quality of cable, then which network provides a better internet connection and why?\n",
    "\tDSL:\n",
    "\t\tDSL stands for digital subscriber line. DSL uses existing phone network to provide internet. DSL provides internet as-well as voice data. Data over DSL goes to the internet and voice goes to the telephone net.\n",
    "\tCable Network:\n",
    "\t\tCable network provides internet via television cable networks. It uses coaxial cables to deliver internet and television data. Cable networks unlike DSL don’t have direct access to the central office.\n",
    "Its primary disadvantage is that you're sharing bandwidth with neighbors who are using the same cable line who can then see your traffic.\n",
    "If both use same type of cables then DSL would be better if it uses coaxial cable.\n",
    "8. What is an ISP? How can a subscriber of an ISB connect to its ISP connection?\n",
    "Explain with an example.\n",
    "\tAn ISP is an internet service provider which provides internet to its customers.  An Internet service provider (ISP) is a company that provides customers with Internet access. Data may be transmitted using several technologies, including dial-up, DSL, cable modem, wireless or dedicated high-speed interconnects.\n",
    "\n",
    "For example a subscriber receives a router/access point to connect to the internet from their home which either uses a telephone line or a fiber line.\n",
    "\n",
    "9. Differentiate between multipoint and point to point connection. What are the advantages of a multipoint connection over a point-to-point connection?  \t\n",
    "When there is a single dedicated link only between two devices, it is a point-to-point connection whereas, if a single link is shared by more than two devices then it is said to be a multipoint connection. In multipoint connection the channel capacity is shared temporarily by the devices in connection.\n",
    "Advantages of multipoint over point to point:\n",
    "The advantages of a multipoint connection over a point-to-point connection are \n",
    "•\tEase of installation\n",
    "•\tLow cost\n",
    "•\tReliability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10. What is the difference between half-duplex and full-duplex transmission modes?\n",
    "Give examples of practical applications of both modes.\n",
    " \t\n",
    "\n",
    "Half-Duplex:\n",
    "\t\tHalf-duplex data transmission of data means that data is transmitted in just one direction at a time. For example, only one person can send data at a time and the other can receive both can’t send data at the same time.\n",
    "Full-Duplex:\n",
    "\tFull-duplex data transmission means that data can be transmitted in both directions on a signal carrier at the same time. For example, on a local area network with full-duplex transmission, one computer can be sending data on the line while another computer is receiving data.\n",
    "\n",
    "\n",
    "\n",
    "11. Compare FDM and TDM techniques in circuit switching with their advantages and dis-advantages. Which technique should be preferred to serve a large number of users and why?\n",
    "\tFDM:\n",
    "\t\tFDM stands for frequency division multiplexing. In this method a single wire is divided into multiple frequencies to send different data.\n",
    "Advantages:\n",
    "\tFDM proves much better latency compared to TDM.\n",
    "Disadvantages:\n",
    "\tLess flexible than TDM allocated frequency cannot be dynamically changes.\n",
    "TDM:\n",
    "\t\tTDM stands for time division multiplexing TDM divides and allocates certain time periods to each channel for sending and receiving data\n",
    "Advantages:\n",
    "TDM has greater flexibility and efficiency, by dynamically allocating more time periods to the signals that need more of the bandwidth, while reducing the time periods to those signals that do not need it.\n",
    "Disadvantages:\n",
    "\tOnly one channel can transmit at a given time and it has more latency\n",
    "\n",
    "FDM performs better for a larger number of users than TDM since each signal uses only a small number of bandwidth at a time so it can accommodate larger users.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "str2 = '''Operating systems can be viewed from two viewpoints resource managers and extended machines. In the resource manager view, the operating system's job is to manage the different parts of the system efficiently. \n",
    "In the extended machine view, the job of the system is to provide the users with abstractions that include processes, address spaces, and files. \n",
    "OS (Operating System) is a System Software, which is used manage computer Resources in Efficient, Reliable and Secure methods. It is the interaction between User and Computer '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_1 = text_cleaner(str1)\n",
    "cleaned_text_2 = text_cleaner(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_1_unique = list(set(cleaned_text_1))\n",
    "cleaned_text_2_unique = list(set(cleaned_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_words = {}\n",
    "matched_synonyms = {}\n",
    "if len(cleaned_text_1_unique) <= len(cleaned_text_2_unique):\n",
    "    for word1 in cleaned_text_1_unique:\n",
    "        for word2 in cleaned_text_2_unique:\n",
    "            if word1 == word2:\n",
    "                matched_words[word1] = word2 \n",
    "            else:\n",
    "                for syn in wordnet.synsets(word2): \n",
    "                    if syn.lemmas():\n",
    "                        for l in syn.lemmas(): \n",
    "                            if word1 == l.name() and word2 != l.name():\n",
    "                                matched_synonyms[word1] = l.name()\n",
    "    prob = len(matched_words.keys()) / len(cleaned_text_1_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count)\n",
    "matched_words\n",
    "matched_synonyms\n",
    "len(list(matched_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matched_words.keys()) / len(cleaned_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "31/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_f1 = {} \n",
    "synonyms_f2 = {} \n",
    "\n",
    "for word in cleaned_text_1:\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.lemmas():\n",
    "            synonyms_f1[word] = syn.lemmas()[0].name() \n",
    "        else:\n",
    "            synonyms_f1[word] = word\n",
    "\n",
    "for word in cleaned_text_2:\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.lemmas():\n",
    "            synonyms_f2[word] = syn.lemmas()[0].name() \n",
    "        else:\n",
    "            synonyms_f2[word] = word\n",
    "\n",
    "            \n",
    "# print(set(synonyms)) \n",
    "print(synonyms_f1)\n",
    "# len(synonyms)\n",
    "print(synonyms_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_synoyms_1 = dict(sorted(synonyms_f1.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_synoyms_2 = dict(sorted(synonyms_f2.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sorted_synoyms_1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word to word match \n",
    "# seq = SequenceMatcher(None,a=list(sorted_synoyms_1.keys()),b=list(sorted_synoyms_2.keys()))\n",
    "# print(seq.ratio())\n",
    "\n",
    "# word to synonyms\n",
    "seq2 = SequenceMatcher(None,a=list(cleaned_pdf_files_with_syn[1].keys()),b=list(cleaned_pdf_files_with_syn[2].values()))\n",
    "print(seq2.ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = SequenceMatcher(None,a=f1,b=f2)\n",
    "print(seq2.ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.real_quick_ratio() # upper bound of plegued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(seq.get_matching_blocks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for block in s.get_matching_blocks():\n",
    "#  |  ...     print(\"a[%d] and b[%d] match for %d elements\" % block)\n",
    "#  |  a[0] and b[0] match for 8 elements\n",
    "#  |  a[8] and b[17] match for 21 elements\n",
    "#  |  a[29] and b[38] match for 0 elements\n",
    "\n",
    "# find_longest_match(alo, ahi, blo, bhi)\n",
    "#  |      Find longest matching block in a[alo:ahi] and b[blo:bhi]\n",
    "\n",
    "# real_quick_ratio()\n",
    "#  |      Return an upper bound on ratio() very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[0:0+25] == f2[0:0+25]\n",
    "print(f1[0:0+25])\n",
    "print(f2[0:0+25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[25:25+22] == f2[26:26+22]\n",
    "print(f1[25:25+22])\n",
    "print(f2[26:26+22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[48:48+0] == f2[49:49+0]\n",
    "print(f1[48:48+0])\n",
    "print(f2[49:49+0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i, j, n)\n",
    "a[i:i+n] == b[j:j+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio(self)\n",
    "#  |      Return a measure of the sequences' similarity (float in [0,1]).\n",
    "#  |      \n",
    "#  |      Where T is the total number of elements in both sequences, and\n",
    "#  |      M is the number of matches, this is 2.0*M / T.\n",
    "#  |      Note that this is 1 if the sequences are identical, and 0 if\n",
    "#  |      they have nothing in common.\n",
    "# match = fuzz.SequenceMatcher(None,a=list(c),b=list(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "\n",
    "for syn in wordnet.synsets(\"good\"): \n",
    "\tfor l in syn.lemmas(): \n",
    "\t\tsynonyms.append(l.name()) \n",
    "\t\tif l.antonyms(): \n",
    "\t\t\tantonyms.append(l.antonyms()[0].name()) \n",
    "\n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = wordnet.synsets(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name_1 = \"Khizar Sultan\"\n",
    "student_name_2 = \"Ali Shahbaz\"\n",
    "\n",
    "word_plagued = \"100%\"\n",
    "syn_plagued = \"26.0%\"\n",
    "\n",
    "list_l = list(\"abcdefghhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = docx.Document()\n",
    "now = datetime.datetime.now()\n",
    "time = now.strftime(\"%Y-%m-%d %I:%M-%p\")\n",
    "document.add_heading(f'Plagued Report',level = 0)\n",
    "document.add_heading(f\"Processed on : {time}\",level=2)\n",
    "document.add_heading(f\"Student Name (who plagued) : {student_name_1}\",level=2)\n",
    "document.add_heading(f\"Student Name (whom plagued) : {student_name_2}\",level=2)\n",
    "document.add_heading(f\"Word Matched Plagued in percentage : {word_plagued}\",level=2)\n",
    "document.add_heading(f\"Synonyms Matched Plagued in percentage : {syn_plagued}\",level=2)\n",
    "\n",
    "document.add_heading('Matched Words:', level=1)\n",
    "document.add_paragraph(f'{list_l}') # , style='Intense Quote')\n",
    "\n",
    "document.add_heading('Matched Synonyms:', level=1)\n",
    "document.add_paragraph(f'{list_l}') # , style='Intense Quote')\n",
    "\n",
    "\n",
    "\n",
    "# document.add_paragraph(\n",
    "#     'first item in unordered list', style='List Bullet'\n",
    "# )\n",
    "# document.add_paragraph(\n",
    "#     'first item in ordered list', style='List Number'\n",
    "# )\n",
    "\n",
    "# records = (\n",
    "#     (3, '101', 'Spam'),\n",
    "#     (7, '422', 'Eggs'),\n",
    "#     (4, '631', 'Spam, spam, eggs, and spam')\n",
    "# )\n",
    "\n",
    "# table = document.add_table(rows=1, cols=3)\n",
    "# hdr_cells = table.rows[0].cells\n",
    "# hdr_cells[0].text = 'Qty'\n",
    "# hdr_cells[1].text = 'Id'\n",
    "# hdr_cells[2].text = 'Desc'\n",
    "# for qty, id, desc in records:\n",
    "#     row_cells = table.add_row().cells\n",
    "#     row_cells[0].text = str(qty)\n",
    "#     row_cells[1].text = id\n",
    "#     row_cells[2].text = desc\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
